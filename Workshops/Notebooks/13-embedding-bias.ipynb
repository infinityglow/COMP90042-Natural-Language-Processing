{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biases in Pretrained Embeddings\n",
    "\n",
    "In this notebook, we'll attempt to build a sentiment classifier, first using pretrained word embeddings (GloVe) and then next using BERT (from Week 7's workshop), and see if these pretrained embeddings/models inherently contain any biases.\n",
    "\n",
    "What is GloVe embeddings? They are word embeddings like Word2Vec, but implemented differently. If you're interested to read more about GloVe, you can find more information [here](https://nlp.stanford.edu/projects/glove/). The crucial thing to note here is that GloVe embeddings are trained on Google News and Common Crawl web data, and so the embeddings themselves are likely to capture common stereotypes and biases in our culture.\n",
    "\n",
    "First, let's upload the GloVe embeddings (\"13-glove.6B.50d.txt\") to your colab instance.\n",
    "\n",
    "Refresher:\n",
    "\n",
    "1. To upload files, click the folder icon on the left, and click the \"upload\" icon to choose files from your local drive (you can also drag and drop files to upload them). Once the files are uploaded, you should see them appearing in the file system.\n",
    "\n",
    "2. Don't forget to enable GPU on the colab notebook. We can do this by going to \"Runtime $>$ Change runtime type\" and selecting \"GPU\" as the hardware accelerator. Click save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get started. Let's load some libraries that we'll be using for building the first sentiment classifier using the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function to load the embeddings using the text file. The format is pretty self-explanatory if you view the embeddings file: it's just one line for each word.\n",
    "\n",
    "Note: the loading process might take a couple seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('13-glove.6B.50d.txt')\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to build a sentiment classifier is to use a sentiment lexicon: a dictionary that contains positive and negative words. There are many sentiment lexicons you could use, but we'll be using the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon). Download them (`13-positive-words.txt` and `13-negative-words.txt` from Canvas) and put them in the same directory of this notebook. Once that's done, we'll load the lexicon using the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'accolade', 'accolades', 'accommodative', 'accomodative', 'accomplish', 'accomplished', 'accomplishment', 'accomplishments', 'accurate', 'accurately', 'achievable', 'achievement', 'achievements', 'achievible', 'acumen', 'adaptable', 'adaptive', 'adequate', 'adjustable', 'admirable']\n",
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'aborts', 'abrade', 'abrasive', 'abrupt', 'abruptly', 'abscond', 'absence', 'absent-minded', 'absentee', 'absurd', 'absurdity', 'absurdly', 'absurdness', 'abuse', 'abused', 'abuses', 'abusive', 'abysmal', 'abysmally', 'abyss']\n"
     ]
    }
   ],
   "source": [
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('13-positive-words.txt')\n",
    "neg_words = load_lexicon('13-negative-words.txt')\n",
    "\n",
    "print(pos_words[:30])\n",
    "print(neg_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we build a sentiment classifier using word embeddings? We'll take a very simple approach, where we embed the positive and negative words using GloVe embeddings, and then train a logistic regression model that predict their sentiments based on their embeddings. Once trained, we can then apply the logistic regression to all other words not part of the lexicon that have a corresponding GloVe embeddings to predict their sentiments.\n",
    "\n",
    "Note: `index.intersection()` is used to only words that are in the GloVe vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abound</th>\n",
       "      <td>0.680940</td>\n",
       "      <td>0.681610</td>\n",
       "      <td>-0.598430</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>-0.695230</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>-0.281520</td>\n",
       "      <td>-0.647090</td>\n",
       "      <td>-0.287200</td>\n",
       "      <td>0.339580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272300</td>\n",
       "      <td>0.341360</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>1.151500</td>\n",
       "      <td>0.907630</td>\n",
       "      <td>0.560140</td>\n",
       "      <td>0.360690</td>\n",
       "      <td>0.685340</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>-0.926680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abounds</th>\n",
       "      <td>0.344170</td>\n",
       "      <td>0.401890</td>\n",
       "      <td>-0.801010</td>\n",
       "      <td>0.605540</td>\n",
       "      <td>-0.364730</td>\n",
       "      <td>-0.517050</td>\n",
       "      <td>0.318030</td>\n",
       "      <td>-0.310820</td>\n",
       "      <td>0.155790</td>\n",
       "      <td>1.076700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196530</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>-0.115510</td>\n",
       "      <td>0.509530</td>\n",
       "      <td>0.152230</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>1.021500</td>\n",
       "      <td>0.450880</td>\n",
       "      <td>-0.938780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abundance</th>\n",
       "      <td>0.135060</td>\n",
       "      <td>1.014300</td>\n",
       "      <td>-0.550050</td>\n",
       "      <td>0.134170</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>-0.061255</td>\n",
       "      <td>-0.110980</td>\n",
       "      <td>-0.482330</td>\n",
       "      <td>0.960090</td>\n",
       "      <td>0.797050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160630</td>\n",
       "      <td>0.026046</td>\n",
       "      <td>-0.084491</td>\n",
       "      <td>0.280180</td>\n",
       "      <td>0.251620</td>\n",
       "      <td>0.048608</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.006417</td>\n",
       "      <td>0.094202</td>\n",
       "      <td>-0.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abundant</th>\n",
       "      <td>0.372980</td>\n",
       "      <td>0.772570</td>\n",
       "      <td>-0.497300</td>\n",
       "      <td>0.272560</td>\n",
       "      <td>0.433190</td>\n",
       "      <td>-0.393870</td>\n",
       "      <td>-0.430560</td>\n",
       "      <td>-0.174180</td>\n",
       "      <td>1.267900</td>\n",
       "      <td>0.874630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368700</td>\n",
       "      <td>0.025354</td>\n",
       "      <td>-0.152660</td>\n",
       "      <td>0.238080</td>\n",
       "      <td>0.404190</td>\n",
       "      <td>0.191850</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>0.048975</td>\n",
       "      <td>-0.080421</td>\n",
       "      <td>-0.523340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accessible</th>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.710430</td>\n",
       "      <td>0.444680</td>\n",
       "      <td>-0.250280</td>\n",
       "      <td>-0.401760</td>\n",
       "      <td>-0.555660</td>\n",
       "      <td>-0.878610</td>\n",
       "      <td>-0.487020</td>\n",
       "      <td>0.800530</td>\n",
       "      <td>0.420210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641040</td>\n",
       "      <td>-0.499470</td>\n",
       "      <td>0.821910</td>\n",
       "      <td>0.969470</td>\n",
       "      <td>-0.729920</td>\n",
       "      <td>-0.031303</td>\n",
       "      <td>-0.210450</td>\n",
       "      <td>-0.544280</td>\n",
       "      <td>0.557920</td>\n",
       "      <td>-0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acclaim</th>\n",
       "      <td>-0.592170</td>\n",
       "      <td>0.527210</td>\n",
       "      <td>-0.231590</td>\n",
       "      <td>-0.582410</td>\n",
       "      <td>-0.376310</td>\n",
       "      <td>-0.012175</td>\n",
       "      <td>-0.427890</td>\n",
       "      <td>0.071798</td>\n",
       "      <td>0.424590</td>\n",
       "      <td>2.103400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544190</td>\n",
       "      <td>0.077834</td>\n",
       "      <td>-1.140700</td>\n",
       "      <td>-0.617610</td>\n",
       "      <td>-1.097100</td>\n",
       "      <td>-0.010793</td>\n",
       "      <td>-0.619540</td>\n",
       "      <td>0.111450</td>\n",
       "      <td>-0.446600</td>\n",
       "      <td>-0.258610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acclaimed</th>\n",
       "      <td>-0.285720</td>\n",
       "      <td>0.444580</td>\n",
       "      <td>-0.745500</td>\n",
       "      <td>-0.376100</td>\n",
       "      <td>-0.317760</td>\n",
       "      <td>0.290250</td>\n",
       "      <td>-0.959750</td>\n",
       "      <td>-0.556490</td>\n",
       "      <td>-0.195140</td>\n",
       "      <td>1.372700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125860</td>\n",
       "      <td>0.518090</td>\n",
       "      <td>-0.321730</td>\n",
       "      <td>-1.057400</td>\n",
       "      <td>-0.261180</td>\n",
       "      <td>0.038646</td>\n",
       "      <td>-0.569100</td>\n",
       "      <td>-0.488770</td>\n",
       "      <td>-0.472750</td>\n",
       "      <td>0.309320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acclamation</th>\n",
       "      <td>-0.931830</td>\n",
       "      <td>-0.040727</td>\n",
       "      <td>-0.132430</td>\n",
       "      <td>-1.023900</td>\n",
       "      <td>-0.256720</td>\n",
       "      <td>-0.207960</td>\n",
       "      <td>0.719530</td>\n",
       "      <td>0.150080</td>\n",
       "      <td>-0.448280</td>\n",
       "      <td>0.258540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269530</td>\n",
       "      <td>0.375180</td>\n",
       "      <td>-1.035900</td>\n",
       "      <td>-0.310760</td>\n",
       "      <td>-1.511000</td>\n",
       "      <td>0.348370</td>\n",
       "      <td>-0.759630</td>\n",
       "      <td>0.079894</td>\n",
       "      <td>0.102110</td>\n",
       "      <td>-0.043891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accolade</th>\n",
       "      <td>-0.784170</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>-0.532170</td>\n",
       "      <td>-0.119030</td>\n",
       "      <td>0.540050</td>\n",
       "      <td>-0.510090</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.376070</td>\n",
       "      <td>0.865570</td>\n",
       "      <td>1.666200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298780</td>\n",
       "      <td>-0.107320</td>\n",
       "      <td>-0.731490</td>\n",
       "      <td>0.473890</td>\n",
       "      <td>-1.216300</td>\n",
       "      <td>-0.187210</td>\n",
       "      <td>-0.152010</td>\n",
       "      <td>0.909840</td>\n",
       "      <td>-0.075292</td>\n",
       "      <td>-0.298360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accolades</th>\n",
       "      <td>-1.118100</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>-0.171850</td>\n",
       "      <td>-0.271550</td>\n",
       "      <td>-0.551700</td>\n",
       "      <td>-0.308900</td>\n",
       "      <td>-0.222280</td>\n",
       "      <td>0.783690</td>\n",
       "      <td>0.341970</td>\n",
       "      <td>1.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140670</td>\n",
       "      <td>0.279610</td>\n",
       "      <td>-0.762770</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>-0.900690</td>\n",
       "      <td>-0.570040</td>\n",
       "      <td>-0.512730</td>\n",
       "      <td>1.218800</td>\n",
       "      <td>-0.715560</td>\n",
       "      <td>-0.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodative</th>\n",
       "      <td>0.142690</td>\n",
       "      <td>0.478190</td>\n",
       "      <td>-0.263490</td>\n",
       "      <td>-2.255300</td>\n",
       "      <td>-0.415190</td>\n",
       "      <td>-1.030400</td>\n",
       "      <td>1.526000</td>\n",
       "      <td>0.052097</td>\n",
       "      <td>-0.570440</td>\n",
       "      <td>-0.074615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027751</td>\n",
       "      <td>-1.745400</td>\n",
       "      <td>0.591680</td>\n",
       "      <td>1.322500</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>-1.053000</td>\n",
       "      <td>0.328850</td>\n",
       "      <td>1.706000</td>\n",
       "      <td>0.567740</td>\n",
       "      <td>0.740830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomodative</th>\n",
       "      <td>-0.159410</td>\n",
       "      <td>-0.219540</td>\n",
       "      <td>-1.061200</td>\n",
       "      <td>-1.759900</td>\n",
       "      <td>-0.579050</td>\n",
       "      <td>-1.234400</td>\n",
       "      <td>1.249200</td>\n",
       "      <td>0.245430</td>\n",
       "      <td>-0.547830</td>\n",
       "      <td>0.093014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109370</td>\n",
       "      <td>-0.819240</td>\n",
       "      <td>0.928060</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>-0.763470</td>\n",
       "      <td>-0.694190</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>1.028400</td>\n",
       "      <td>0.144890</td>\n",
       "      <td>0.792710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomplish</th>\n",
       "      <td>0.637390</td>\n",
       "      <td>-0.272200</td>\n",
       "      <td>0.242420</td>\n",
       "      <td>-0.392480</td>\n",
       "      <td>0.611170</td>\n",
       "      <td>-0.233080</td>\n",
       "      <td>0.214820</td>\n",
       "      <td>0.281210</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.324320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.826370</td>\n",
       "      <td>-0.992620</td>\n",
       "      <td>0.066650</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>-0.624190</td>\n",
       "      <td>0.049727</td>\n",
       "      <td>0.400690</td>\n",
       "      <td>0.704640</td>\n",
       "      <td>-0.334690</td>\n",
       "      <td>0.895320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomplished</th>\n",
       "      <td>-0.343680</td>\n",
       "      <td>0.130460</td>\n",
       "      <td>-0.265460</td>\n",
       "      <td>-0.573130</td>\n",
       "      <td>0.470830</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>-0.564550</td>\n",
       "      <td>0.474320</td>\n",
       "      <td>-0.422350</td>\n",
       "      <td>0.681910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285580</td>\n",
       "      <td>-0.258390</td>\n",
       "      <td>-0.033096</td>\n",
       "      <td>0.074728</td>\n",
       "      <td>-0.340410</td>\n",
       "      <td>-0.503850</td>\n",
       "      <td>-0.069134</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>-0.678190</td>\n",
       "      <td>0.812130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomplishment</th>\n",
       "      <td>-0.309950</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>-0.618860</td>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.254460</td>\n",
       "      <td>0.658040</td>\n",
       "      <td>0.398090</td>\n",
       "      <td>0.782020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861410</td>\n",
       "      <td>-0.606870</td>\n",
       "      <td>-0.945150</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>-0.955460</td>\n",
       "      <td>0.076932</td>\n",
       "      <td>-0.143450</td>\n",
       "      <td>1.073900</td>\n",
       "      <td>-0.292720</td>\n",
       "      <td>0.343090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomplishments</th>\n",
       "      <td>-0.744990</td>\n",
       "      <td>1.217200</td>\n",
       "      <td>-0.367500</td>\n",
       "      <td>-0.194370</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>-0.080197</td>\n",
       "      <td>-0.385500</td>\n",
       "      <td>0.094538</td>\n",
       "      <td>0.169260</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.931110</td>\n",
       "      <td>-0.397700</td>\n",
       "      <td>-0.714830</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>-0.736050</td>\n",
       "      <td>-0.262070</td>\n",
       "      <td>-0.276130</td>\n",
       "      <td>0.933830</td>\n",
       "      <td>-0.475750</td>\n",
       "      <td>-0.136320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accurate</th>\n",
       "      <td>0.110420</td>\n",
       "      <td>-0.794960</td>\n",
       "      <td>0.640880</td>\n",
       "      <td>-0.428650</td>\n",
       "      <td>0.808670</td>\n",
       "      <td>-0.276890</td>\n",
       "      <td>0.114350</td>\n",
       "      <td>-0.356810</td>\n",
       "      <td>0.724150</td>\n",
       "      <td>0.476020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>-0.035198</td>\n",
       "      <td>0.278290</td>\n",
       "      <td>0.499780</td>\n",
       "      <td>-0.567040</td>\n",
       "      <td>-0.382330</td>\n",
       "      <td>0.709740</td>\n",
       "      <td>1.624500</td>\n",
       "      <td>0.973590</td>\n",
       "      <td>0.608770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accurately</th>\n",
       "      <td>0.460030</td>\n",
       "      <td>-0.955770</td>\n",
       "      <td>1.083700</td>\n",
       "      <td>-0.672890</td>\n",
       "      <td>0.681280</td>\n",
       "      <td>-0.081830</td>\n",
       "      <td>0.517630</td>\n",
       "      <td>-0.177270</td>\n",
       "      <td>0.211550</td>\n",
       "      <td>-0.049742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050443</td>\n",
       "      <td>-0.312260</td>\n",
       "      <td>0.481920</td>\n",
       "      <td>0.156350</td>\n",
       "      <td>-0.339780</td>\n",
       "      <td>-0.517530</td>\n",
       "      <td>0.779910</td>\n",
       "      <td>0.814850</td>\n",
       "      <td>0.528920</td>\n",
       "      <td>0.327770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achievable</th>\n",
       "      <td>0.748450</td>\n",
       "      <td>-0.425380</td>\n",
       "      <td>0.896620</td>\n",
       "      <td>-1.137100</td>\n",
       "      <td>0.858280</td>\n",
       "      <td>-0.381090</td>\n",
       "      <td>0.815290</td>\n",
       "      <td>-0.888270</td>\n",
       "      <td>0.361790</td>\n",
       "      <td>1.230600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359270</td>\n",
       "      <td>-0.645960</td>\n",
       "      <td>0.445690</td>\n",
       "      <td>-0.202140</td>\n",
       "      <td>-1.370600</td>\n",
       "      <td>-0.226760</td>\n",
       "      <td>0.573670</td>\n",
       "      <td>1.253200</td>\n",
       "      <td>0.687960</td>\n",
       "      <td>0.977740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achievement</th>\n",
       "      <td>-0.937190</td>\n",
       "      <td>1.547000</td>\n",
       "      <td>-0.537040</td>\n",
       "      <td>-0.372400</td>\n",
       "      <td>0.640320</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.287790</td>\n",
       "      <td>-0.376550</td>\n",
       "      <td>0.858010</td>\n",
       "      <td>0.776500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766860</td>\n",
       "      <td>-0.500740</td>\n",
       "      <td>-0.487560</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>-1.008000</td>\n",
       "      <td>-0.191870</td>\n",
       "      <td>0.304330</td>\n",
       "      <td>0.975660</td>\n",
       "      <td>-0.224780</td>\n",
       "      <td>0.271330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achievements</th>\n",
       "      <td>-0.519330</td>\n",
       "      <td>1.366800</td>\n",
       "      <td>-0.687400</td>\n",
       "      <td>0.095220</td>\n",
       "      <td>0.186070</td>\n",
       "      <td>-0.333860</td>\n",
       "      <td>-0.050129</td>\n",
       "      <td>-0.402980</td>\n",
       "      <td>0.333730</td>\n",
       "      <td>0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977470</td>\n",
       "      <td>-0.649180</td>\n",
       "      <td>-0.282470</td>\n",
       "      <td>-0.349090</td>\n",
       "      <td>-0.505080</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>-0.496300</td>\n",
       "      <td>1.054500</td>\n",
       "      <td>-0.194040</td>\n",
       "      <td>-0.709510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acumen</th>\n",
       "      <td>-0.230750</td>\n",
       "      <td>-0.179580</td>\n",
       "      <td>-0.404290</td>\n",
       "      <td>-0.029111</td>\n",
       "      <td>-0.102500</td>\n",
       "      <td>-0.313560</td>\n",
       "      <td>0.363180</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>-0.056340</td>\n",
       "      <td>0.627770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329710</td>\n",
       "      <td>-0.234590</td>\n",
       "      <td>-0.824520</td>\n",
       "      <td>1.226400</td>\n",
       "      <td>-0.415110</td>\n",
       "      <td>-0.709680</td>\n",
       "      <td>0.244940</td>\n",
       "      <td>1.059100</td>\n",
       "      <td>-0.168230</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaptable</th>\n",
       "      <td>0.740600</td>\n",
       "      <td>-0.967460</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>-0.512930</td>\n",
       "      <td>0.194910</td>\n",
       "      <td>-0.109880</td>\n",
       "      <td>0.449280</td>\n",
       "      <td>-0.744890</td>\n",
       "      <td>-0.172240</td>\n",
       "      <td>0.389840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066087</td>\n",
       "      <td>-0.382870</td>\n",
       "      <td>0.401890</td>\n",
       "      <td>1.087200</td>\n",
       "      <td>1.033700</td>\n",
       "      <td>0.130920</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.381350</td>\n",
       "      <td>-0.606350</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaptive</th>\n",
       "      <td>0.810480</td>\n",
       "      <td>0.020273</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>0.740840</td>\n",
       "      <td>-0.834880</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.973240</td>\n",
       "      <td>-1.660900</td>\n",
       "      <td>0.283130</td>\n",
       "      <td>0.704280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351570</td>\n",
       "      <td>-0.176990</td>\n",
       "      <td>-0.586910</td>\n",
       "      <td>1.517000</td>\n",
       "      <td>-0.181400</td>\n",
       "      <td>-0.625510</td>\n",
       "      <td>1.179500</td>\n",
       "      <td>0.268520</td>\n",
       "      <td>0.480960</td>\n",
       "      <td>0.716960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adequate</th>\n",
       "      <td>0.297050</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>0.140930</td>\n",
       "      <td>-0.632550</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>-0.464230</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>0.325410</td>\n",
       "      <td>1.509100</td>\n",
       "      <td>0.219310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>-0.393190</td>\n",
       "      <td>0.599050</td>\n",
       "      <td>0.610890</td>\n",
       "      <td>-0.475770</td>\n",
       "      <td>-0.162280</td>\n",
       "      <td>-0.483220</td>\n",
       "      <td>1.378600</td>\n",
       "      <td>0.526970</td>\n",
       "      <td>0.733280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjustable</th>\n",
       "      <td>0.057434</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>1.441700</td>\n",
       "      <td>-1.097500</td>\n",
       "      <td>-0.357510</td>\n",
       "      <td>1.350700</td>\n",
       "      <td>0.670450</td>\n",
       "      <td>-0.685100</td>\n",
       "      <td>-0.602070</td>\n",
       "      <td>0.367160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643070</td>\n",
       "      <td>0.778840</td>\n",
       "      <td>-0.422750</td>\n",
       "      <td>1.624400</td>\n",
       "      <td>-0.820810</td>\n",
       "      <td>-0.914660</td>\n",
       "      <td>1.589200</td>\n",
       "      <td>0.139630</td>\n",
       "      <td>0.432760</td>\n",
       "      <td>-0.680860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admirable</th>\n",
       "      <td>-0.127900</td>\n",
       "      <td>-0.090377</td>\n",
       "      <td>-0.723990</td>\n",
       "      <td>-0.791190</td>\n",
       "      <td>1.052100</td>\n",
       "      <td>0.181040</td>\n",
       "      <td>0.639720</td>\n",
       "      <td>-0.308440</td>\n",
       "      <td>0.231720</td>\n",
       "      <td>0.895850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230230</td>\n",
       "      <td>-0.276790</td>\n",
       "      <td>-0.090644</td>\n",
       "      <td>0.094873</td>\n",
       "      <td>-0.453070</td>\n",
       "      <td>-0.066544</td>\n",
       "      <td>-0.082008</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.146630</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admirably</th>\n",
       "      <td>0.376790</td>\n",
       "      <td>-1.148200</td>\n",
       "      <td>-0.940230</td>\n",
       "      <td>-1.214000</td>\n",
       "      <td>0.162860</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.444170</td>\n",
       "      <td>0.486970</td>\n",
       "      <td>-0.259090</td>\n",
       "      <td>0.625060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196870</td>\n",
       "      <td>-0.423700</td>\n",
       "      <td>-0.408390</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>-0.399630</td>\n",
       "      <td>-0.001001</td>\n",
       "      <td>-0.104410</td>\n",
       "      <td>-0.142980</td>\n",
       "      <td>-0.547960</td>\n",
       "      <td>0.754670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>-0.153550</td>\n",
       "      <td>1.400100</td>\n",
       "      <td>-0.562520</td>\n",
       "      <td>-0.674800</td>\n",
       "      <td>0.766420</td>\n",
       "      <td>0.188550</td>\n",
       "      <td>0.387360</td>\n",
       "      <td>0.582840</td>\n",
       "      <td>-0.755210</td>\n",
       "      <td>1.265200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905440</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>-0.768660</td>\n",
       "      <td>0.052548</td>\n",
       "      <td>0.117720</td>\n",
       "      <td>-0.366380</td>\n",
       "      <td>-0.845940</td>\n",
       "      <td>0.319340</td>\n",
       "      <td>-0.905260</td>\n",
       "      <td>-0.587110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admire</th>\n",
       "      <td>0.045975</td>\n",
       "      <td>0.456580</td>\n",
       "      <td>0.114950</td>\n",
       "      <td>-0.819460</td>\n",
       "      <td>0.847230</td>\n",
       "      <td>-0.584460</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.096512</td>\n",
       "      <td>-0.708970</td>\n",
       "      <td>0.685690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114870</td>\n",
       "      <td>0.209020</td>\n",
       "      <td>0.054165</td>\n",
       "      <td>0.217130</td>\n",
       "      <td>0.215080</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.341700</td>\n",
       "      <td>-0.318710</td>\n",
       "      <td>-0.681040</td>\n",
       "      <td>0.169950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>-0.117550</td>\n",
       "      <td>0.637350</td>\n",
       "      <td>-0.532240</td>\n",
       "      <td>-0.609040</td>\n",
       "      <td>1.283700</td>\n",
       "      <td>-0.087655</td>\n",
       "      <td>0.482680</td>\n",
       "      <td>-0.365990</td>\n",
       "      <td>0.205640</td>\n",
       "      <td>0.614630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270910</td>\n",
       "      <td>-0.286990</td>\n",
       "      <td>-0.384710</td>\n",
       "      <td>1.244600</td>\n",
       "      <td>-0.056028</td>\n",
       "      <td>0.706640</td>\n",
       "      <td>0.246510</td>\n",
       "      <td>0.448580</td>\n",
       "      <td>-0.481630</td>\n",
       "      <td>-0.519550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wise</th>\n",
       "      <td>-0.198430</td>\n",
       "      <td>0.262610</td>\n",
       "      <td>-0.314930</td>\n",
       "      <td>-0.705050</td>\n",
       "      <td>1.085300</td>\n",
       "      <td>-0.434220</td>\n",
       "      <td>-0.173200</td>\n",
       "      <td>-0.127520</td>\n",
       "      <td>-0.498940</td>\n",
       "      <td>0.138880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098422</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>-0.038803</td>\n",
       "      <td>0.308120</td>\n",
       "      <td>0.026377</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>-0.139110</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>0.441250</td>\n",
       "      <td>1.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisely</th>\n",
       "      <td>0.236190</td>\n",
       "      <td>-1.085600</td>\n",
       "      <td>0.165910</td>\n",
       "      <td>-0.980650</td>\n",
       "      <td>0.430790</td>\n",
       "      <td>-0.345240</td>\n",
       "      <td>-0.034623</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>-0.325040</td>\n",
       "      <td>0.402230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303710</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>-0.116600</td>\n",
       "      <td>0.424730</td>\n",
       "      <td>-0.955570</td>\n",
       "      <td>-0.225760</td>\n",
       "      <td>-0.331410</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-0.456060</td>\n",
       "      <td>0.693280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>witty</th>\n",
       "      <td>-0.386800</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>-1.020500</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.496050</td>\n",
       "      <td>0.679690</td>\n",
       "      <td>0.297560</td>\n",
       "      <td>-0.558480</td>\n",
       "      <td>-0.842530</td>\n",
       "      <td>0.823090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363900</td>\n",
       "      <td>0.634950</td>\n",
       "      <td>0.147240</td>\n",
       "      <td>0.290440</td>\n",
       "      <td>0.661010</td>\n",
       "      <td>-0.192140</td>\n",
       "      <td>0.422710</td>\n",
       "      <td>0.215890</td>\n",
       "      <td>0.349530</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won</th>\n",
       "      <td>-1.556100</td>\n",
       "      <td>0.862410</td>\n",
       "      <td>0.146040</td>\n",
       "      <td>1.138900</td>\n",
       "      <td>-0.168750</td>\n",
       "      <td>0.689050</td>\n",
       "      <td>-0.641420</td>\n",
       "      <td>0.406180</td>\n",
       "      <td>-0.887950</td>\n",
       "      <td>0.065865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111360</td>\n",
       "      <td>-0.082122</td>\n",
       "      <td>-0.043902</td>\n",
       "      <td>-0.527590</td>\n",
       "      <td>-0.730330</td>\n",
       "      <td>0.202930</td>\n",
       "      <td>-0.824480</td>\n",
       "      <td>-0.605930</td>\n",
       "      <td>-0.061551</td>\n",
       "      <td>-0.488980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.273280</td>\n",
       "      <td>0.299040</td>\n",
       "      <td>-0.448740</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>-0.116580</td>\n",
       "      <td>-0.744220</td>\n",
       "      <td>0.331040</td>\n",
       "      <td>-0.198380</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>0.108370</td>\n",
       "      <td>-0.487210</td>\n",
       "      <td>0.221040</td>\n",
       "      <td>0.304720</td>\n",
       "      <td>0.515760</td>\n",
       "      <td>0.026934</td>\n",
       "      <td>-0.339080</td>\n",
       "      <td>-0.515170</td>\n",
       "      <td>0.394230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>0.235330</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>-1.200800</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>1.284300</td>\n",
       "      <td>-0.104950</td>\n",
       "      <td>-0.469280</td>\n",
       "      <td>-0.160640</td>\n",
       "      <td>-0.002305</td>\n",
       "      <td>0.882190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204910</td>\n",
       "      <td>0.095888</td>\n",
       "      <td>-0.396820</td>\n",
       "      <td>-0.187090</td>\n",
       "      <td>0.219040</td>\n",
       "      <td>0.576940</td>\n",
       "      <td>0.066142</td>\n",
       "      <td>-0.161620</td>\n",
       "      <td>-0.024670</td>\n",
       "      <td>0.846260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderfully</th>\n",
       "      <td>0.635230</td>\n",
       "      <td>-0.430960</td>\n",
       "      <td>-1.242100</td>\n",
       "      <td>-0.475730</td>\n",
       "      <td>0.717270</td>\n",
       "      <td>0.092392</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>-0.214020</td>\n",
       "      <td>-0.427690</td>\n",
       "      <td>1.257000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340470</td>\n",
       "      <td>0.336160</td>\n",
       "      <td>-0.089202</td>\n",
       "      <td>-0.027039</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.217150</td>\n",
       "      <td>-0.211800</td>\n",
       "      <td>-0.121790</td>\n",
       "      <td>0.851340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonders</th>\n",
       "      <td>0.163040</td>\n",
       "      <td>0.569870</td>\n",
       "      <td>-0.050685</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.537940</td>\n",
       "      <td>-0.334660</td>\n",
       "      <td>-0.627000</td>\n",
       "      <td>0.090018</td>\n",
       "      <td>0.171250</td>\n",
       "      <td>0.341670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452470</td>\n",
       "      <td>-0.114100</td>\n",
       "      <td>-0.098827</td>\n",
       "      <td>0.314720</td>\n",
       "      <td>-0.054529</td>\n",
       "      <td>0.552650</td>\n",
       "      <td>0.186910</td>\n",
       "      <td>-0.106790</td>\n",
       "      <td>-0.294150</td>\n",
       "      <td>0.129970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wondrous</th>\n",
       "      <td>0.776470</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>-0.938690</td>\n",
       "      <td>0.156170</td>\n",
       "      <td>0.506070</td>\n",
       "      <td>-0.664550</td>\n",
       "      <td>0.317310</td>\n",
       "      <td>-0.084731</td>\n",
       "      <td>0.339480</td>\n",
       "      <td>1.135600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314790</td>\n",
       "      <td>-0.047893</td>\n",
       "      <td>-0.053442</td>\n",
       "      <td>0.287780</td>\n",
       "      <td>-0.415410</td>\n",
       "      <td>0.776940</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>-0.292090</td>\n",
       "      <td>0.106880</td>\n",
       "      <td>-0.131570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woo</th>\n",
       "      <td>0.209310</td>\n",
       "      <td>0.226970</td>\n",
       "      <td>1.148400</td>\n",
       "      <td>0.010467</td>\n",
       "      <td>-0.684760</td>\n",
       "      <td>-0.144020</td>\n",
       "      <td>0.042576</td>\n",
       "      <td>0.029019</td>\n",
       "      <td>-0.646400</td>\n",
       "      <td>0.858580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403070</td>\n",
       "      <td>0.632570</td>\n",
       "      <td>0.116870</td>\n",
       "      <td>-0.218940</td>\n",
       "      <td>1.562700</td>\n",
       "      <td>-1.300100</td>\n",
       "      <td>-0.180990</td>\n",
       "      <td>-0.436060</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.549410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.513590</td>\n",
       "      <td>0.196950</td>\n",
       "      <td>-0.519440</td>\n",
       "      <td>-0.862180</td>\n",
       "      <td>0.015494</td>\n",
       "      <td>0.109730</td>\n",
       "      <td>-0.802930</td>\n",
       "      <td>-0.333610</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159150</td>\n",
       "      <td>-0.304380</td>\n",
       "      <td>0.160250</td>\n",
       "      <td>-0.182900</td>\n",
       "      <td>-0.038563</td>\n",
       "      <td>-0.176190</td>\n",
       "      <td>0.027041</td>\n",
       "      <td>0.046842</td>\n",
       "      <td>-0.628970</td>\n",
       "      <td>0.357260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workable</th>\n",
       "      <td>0.971330</td>\n",
       "      <td>-0.939170</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>-0.092548</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>0.995560</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>-0.522460</td>\n",
       "      <td>-0.314470</td>\n",
       "      <td>0.454140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072538</td>\n",
       "      <td>-1.042900</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>0.382360</td>\n",
       "      <td>-0.612450</td>\n",
       "      <td>0.317240</td>\n",
       "      <td>0.348660</td>\n",
       "      <td>1.518300</td>\n",
       "      <td>0.532910</td>\n",
       "      <td>0.547180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worked</th>\n",
       "      <td>-0.050169</td>\n",
       "      <td>-0.219080</td>\n",
       "      <td>-0.302020</td>\n",
       "      <td>-0.595900</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>0.049210</td>\n",
       "      <td>-1.690100</td>\n",
       "      <td>0.059931</td>\n",
       "      <td>-0.434210</td>\n",
       "      <td>-0.324250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106040</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>-0.061076</td>\n",
       "      <td>-0.214920</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>-0.336180</td>\n",
       "      <td>-0.411700</td>\n",
       "      <td>-0.576980</td>\n",
       "      <td>-0.647930</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>works</th>\n",
       "      <td>0.562660</td>\n",
       "      <td>0.590510</td>\n",
       "      <td>-0.728210</td>\n",
       "      <td>-0.659680</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.412020</td>\n",
       "      <td>-0.821940</td>\n",
       "      <td>-0.832490</td>\n",
       "      <td>-0.366390</td>\n",
       "      <td>0.601040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546250</td>\n",
       "      <td>-0.047358</td>\n",
       "      <td>0.225150</td>\n",
       "      <td>-0.355740</td>\n",
       "      <td>-0.057102</td>\n",
       "      <td>0.326410</td>\n",
       "      <td>-0.265200</td>\n",
       "      <td>-0.227130</td>\n",
       "      <td>-0.270250</td>\n",
       "      <td>0.131650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world-famous</th>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.705080</td>\n",
       "      <td>-1.001900</td>\n",
       "      <td>0.214820</td>\n",
       "      <td>-0.629170</td>\n",
       "      <td>-0.073151</td>\n",
       "      <td>-0.597170</td>\n",
       "      <td>0.484850</td>\n",
       "      <td>0.038725</td>\n",
       "      <td>0.558310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031396</td>\n",
       "      <td>-0.203950</td>\n",
       "      <td>-0.672850</td>\n",
       "      <td>0.370890</td>\n",
       "      <td>-0.022468</td>\n",
       "      <td>-0.368070</td>\n",
       "      <td>-0.246010</td>\n",
       "      <td>-0.418890</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>-0.603020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.387850</td>\n",
       "      <td>0.533970</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>1.059200</td>\n",
       "      <td>-0.197710</td>\n",
       "      <td>-0.737480</td>\n",
       "      <td>-0.964010</td>\n",
       "      <td>0.104160</td>\n",
       "      <td>0.643090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>0.018070</td>\n",
       "      <td>1.131600</td>\n",
       "      <td>0.384960</td>\n",
       "      <td>-0.742530</td>\n",
       "      <td>0.087778</td>\n",
       "      <td>-0.711340</td>\n",
       "      <td>0.847190</td>\n",
       "      <td>-0.557230</td>\n",
       "      <td>0.300220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worthiness</th>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.371510</td>\n",
       "      <td>0.051259</td>\n",
       "      <td>-0.491650</td>\n",
       "      <td>-0.802410</td>\n",
       "      <td>-0.047907</td>\n",
       "      <td>0.449370</td>\n",
       "      <td>0.513280</td>\n",
       "      <td>1.646700</td>\n",
       "      <td>0.117020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>-0.429740</td>\n",
       "      <td>0.813310</td>\n",
       "      <td>0.862590</td>\n",
       "      <td>-0.921880</td>\n",
       "      <td>-0.367720</td>\n",
       "      <td>0.463110</td>\n",
       "      <td>1.107800</td>\n",
       "      <td>0.756580</td>\n",
       "      <td>-0.456670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worthwhile</th>\n",
       "      <td>1.135300</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>-0.224030</td>\n",
       "      <td>-0.295590</td>\n",
       "      <td>0.255210</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.014768</td>\n",
       "      <td>0.195690</td>\n",
       "      <td>0.594270</td>\n",
       "      <td>1.161200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257890</td>\n",
       "      <td>-0.834040</td>\n",
       "      <td>-0.051418</td>\n",
       "      <td>0.705190</td>\n",
       "      <td>-0.140910</td>\n",
       "      <td>-0.245680</td>\n",
       "      <td>-0.044524</td>\n",
       "      <td>1.161500</td>\n",
       "      <td>0.190320</td>\n",
       "      <td>1.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worthy</th>\n",
       "      <td>0.307940</td>\n",
       "      <td>0.944360</td>\n",
       "      <td>-0.463820</td>\n",
       "      <td>-0.205540</td>\n",
       "      <td>0.833860</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.243570</td>\n",
       "      <td>0.355880</td>\n",
       "      <td>0.134050</td>\n",
       "      <td>0.958150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169690</td>\n",
       "      <td>-0.178250</td>\n",
       "      <td>-0.358940</td>\n",
       "      <td>0.508950</td>\n",
       "      <td>-0.331540</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>-0.749030</td>\n",
       "      <td>0.272470</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.562940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wow</th>\n",
       "      <td>-0.363620</td>\n",
       "      <td>0.267590</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>-0.377840</td>\n",
       "      <td>0.190680</td>\n",
       "      <td>-1.199800</td>\n",
       "      <td>0.213720</td>\n",
       "      <td>-0.074527</td>\n",
       "      <td>-0.062496</td>\n",
       "      <td>1.102700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.583550</td>\n",
       "      <td>-0.728800</td>\n",
       "      <td>-0.470480</td>\n",
       "      <td>0.535540</td>\n",
       "      <td>-0.134690</td>\n",
       "      <td>0.263770</td>\n",
       "      <td>-0.175680</td>\n",
       "      <td>-0.084818</td>\n",
       "      <td>1.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wowed</th>\n",
       "      <td>-0.499150</td>\n",
       "      <td>-0.868430</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>-0.747580</td>\n",
       "      <td>0.081121</td>\n",
       "      <td>-1.083300</td>\n",
       "      <td>-0.347260</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>-0.660840</td>\n",
       "      <td>0.953610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110540</td>\n",
       "      <td>0.485320</td>\n",
       "      <td>-0.460860</td>\n",
       "      <td>0.265750</td>\n",
       "      <td>0.474920</td>\n",
       "      <td>-0.178660</td>\n",
       "      <td>-0.236320</td>\n",
       "      <td>-0.216050</td>\n",
       "      <td>-0.619110</td>\n",
       "      <td>0.004770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wowing</th>\n",
       "      <td>0.341400</td>\n",
       "      <td>-0.856290</td>\n",
       "      <td>0.146980</td>\n",
       "      <td>-1.193500</td>\n",
       "      <td>-0.072555</td>\n",
       "      <td>-0.667440</td>\n",
       "      <td>0.306280</td>\n",
       "      <td>0.394040</td>\n",
       "      <td>-0.185790</td>\n",
       "      <td>1.232600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.280340</td>\n",
       "      <td>-0.246630</td>\n",
       "      <td>0.735830</td>\n",
       "      <td>0.513840</td>\n",
       "      <td>-0.066444</td>\n",
       "      <td>0.191680</td>\n",
       "      <td>0.095857</td>\n",
       "      <td>-0.711380</td>\n",
       "      <td>-0.466050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wows</th>\n",
       "      <td>-0.271800</td>\n",
       "      <td>0.155680</td>\n",
       "      <td>0.469190</td>\n",
       "      <td>-0.688140</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>-1.181800</td>\n",
       "      <td>0.552030</td>\n",
       "      <td>0.226460</td>\n",
       "      <td>-0.642360</td>\n",
       "      <td>0.437320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436350</td>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.194060</td>\n",
       "      <td>0.539290</td>\n",
       "      <td>-0.834520</td>\n",
       "      <td>0.510680</td>\n",
       "      <td>0.269550</td>\n",
       "      <td>-1.039900</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yay</th>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.822920</td>\n",
       "      <td>1.449100</td>\n",
       "      <td>-2.031600</td>\n",
       "      <td>-0.970630</td>\n",
       "      <td>-1.893900</td>\n",
       "      <td>0.766980</td>\n",
       "      <td>-0.881990</td>\n",
       "      <td>0.550140</td>\n",
       "      <td>1.321500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>2.108600</td>\n",
       "      <td>0.309790</td>\n",
       "      <td>-2.679000</td>\n",
       "      <td>0.208180</td>\n",
       "      <td>0.306560</td>\n",
       "      <td>-0.249580</td>\n",
       "      <td>-1.835000</td>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.336040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youthful</th>\n",
       "      <td>-0.066812</td>\n",
       "      <td>0.576050</td>\n",
       "      <td>-0.173320</td>\n",
       "      <td>-0.948570</td>\n",
       "      <td>0.707060</td>\n",
       "      <td>0.417810</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>-0.236360</td>\n",
       "      <td>-0.654950</td>\n",
       "      <td>1.599600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105450</td>\n",
       "      <td>0.061121</td>\n",
       "      <td>-0.535190</td>\n",
       "      <td>0.206480</td>\n",
       "      <td>0.465120</td>\n",
       "      <td>-0.115180</td>\n",
       "      <td>-0.127490</td>\n",
       "      <td>-0.375420</td>\n",
       "      <td>0.066607</td>\n",
       "      <td>-0.142230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>-0.122570</td>\n",
       "      <td>-0.566770</td>\n",
       "      <td>-0.662470</td>\n",
       "      <td>-1.667400</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.511770</td>\n",
       "      <td>-0.152310</td>\n",
       "      <td>0.282440</td>\n",
       "      <td>1.104500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172280</td>\n",
       "      <td>-0.260910</td>\n",
       "      <td>-0.890120</td>\n",
       "      <td>0.849820</td>\n",
       "      <td>0.060605</td>\n",
       "      <td>-0.132610</td>\n",
       "      <td>-0.289160</td>\n",
       "      <td>0.552470</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>-0.378880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zenith</th>\n",
       "      <td>0.119250</td>\n",
       "      <td>0.384110</td>\n",
       "      <td>0.808770</td>\n",
       "      <td>0.539840</td>\n",
       "      <td>-0.592570</td>\n",
       "      <td>-0.456540</td>\n",
       "      <td>0.073042</td>\n",
       "      <td>-0.270610</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>0.462640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821540</td>\n",
       "      <td>-0.261640</td>\n",
       "      <td>-0.440520</td>\n",
       "      <td>0.506450</td>\n",
       "      <td>-1.026300</td>\n",
       "      <td>-0.219950</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>-0.098213</td>\n",
       "      <td>-0.839860</td>\n",
       "      <td>-0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0.167720</td>\n",
       "      <td>-0.183480</td>\n",
       "      <td>-0.832810</td>\n",
       "      <td>0.237150</td>\n",
       "      <td>0.720410</td>\n",
       "      <td>0.760570</td>\n",
       "      <td>0.714570</td>\n",
       "      <td>0.223430</td>\n",
       "      <td>-0.360100</td>\n",
       "      <td>0.651780</td>\n",
       "      <td>...</td>\n",
       "      <td>1.312400</td>\n",
       "      <td>-0.489870</td>\n",
       "      <td>-0.815220</td>\n",
       "      <td>-0.475070</td>\n",
       "      <td>-0.279160</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>0.102270</td>\n",
       "      <td>0.090781</td>\n",
       "      <td>-0.087266</td>\n",
       "      <td>-0.076426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zippy</th>\n",
       "      <td>0.333870</td>\n",
       "      <td>-0.830000</td>\n",
       "      <td>-0.008908</td>\n",
       "      <td>-0.047370</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.130150</td>\n",
       "      <td>-0.521620</td>\n",
       "      <td>-0.054453</td>\n",
       "      <td>0.869590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541260</td>\n",
       "      <td>0.260410</td>\n",
       "      <td>-0.919060</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>-0.329930</td>\n",
       "      <td>-0.176470</td>\n",
       "      <td>0.941670</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.528820</td>\n",
       "      <td>0.706110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1893 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0        1         2        3         4         5   \\\n",
       "abound      0.680940  0.68161 -0.598430  0.45290 -0.695230  0.024194   \n",
       "abounds     0.344170  0.40189 -0.801010  0.60554 -0.364730 -0.517050   \n",
       "abundance   0.135060  1.01430 -0.550050  0.13417  0.714800 -0.061255   \n",
       "abundant    0.372980  0.77257 -0.497300  0.27256  0.433190 -0.393870   \n",
       "accessible  0.746860  0.71043  0.444680 -0.25028 -0.401760 -0.555660   \n",
       "...              ...      ...       ...      ...       ...       ...   \n",
       "youthful   -0.066812  0.57605 -0.173320 -0.94857  0.707060  0.417810   \n",
       "zeal       -0.122570 -0.56677 -0.662470 -1.66740  0.542580  0.004206   \n",
       "zenith      0.119250  0.38411  0.808770  0.53984 -0.592570 -0.456540   \n",
       "zest        0.167720 -0.18348 -0.832810  0.23715  0.720410  0.760570   \n",
       "zippy       0.333870 -0.83000 -0.008908 -0.04737 -0.014258  0.020882   \n",
       "\n",
       "                  6        7         8        9   ...       40        41  \\\n",
       "abound     -0.281520 -0.64709 -0.287200  0.33958  ... -0.27230  0.341360   \n",
       "abounds     0.318030 -0.31082  0.155790  1.07670  ...  0.19653  0.335710   \n",
       "abundance  -0.110980 -0.48233  0.960090  0.79705  ... -0.16063  0.026046   \n",
       "abundant   -0.430560 -0.17418  1.267900  0.87463  ... -0.36870  0.025354   \n",
       "accessible -0.878610 -0.48702  0.800530  0.42021  ...  0.64104 -0.499470   \n",
       "...              ...      ...       ...      ...  ...      ...       ...   \n",
       "youthful    0.212500 -0.23636 -0.654950  1.59960  ... -0.10545  0.061121   \n",
       "zeal        0.511770 -0.15231  0.282440  1.10450  ...  0.17228 -0.260910   \n",
       "zenith      0.073042 -0.27061  0.041110  0.46264  ...  0.82154 -0.261640   \n",
       "zest        0.714570  0.22343 -0.360100  0.65178  ...  1.31240 -0.489870   \n",
       "zippy       0.130150 -0.52162 -0.054453  0.86959  ...  0.54126  0.260410   \n",
       "\n",
       "                  42       43        44        45       46        47  \\\n",
       "abound      0.526700  1.15150  0.907630  0.560140  0.36069  0.685340   \n",
       "abounds    -0.115510  0.50953  0.152230  0.601010  0.45070  1.021500   \n",
       "abundance  -0.084491  0.28018  0.251620  0.048608 -0.00686 -0.006417   \n",
       "abundant   -0.152660  0.23808  0.404190  0.191850 -0.50327  0.048975   \n",
       "accessible  0.821910  0.96947 -0.729920 -0.031303 -0.21045 -0.544280   \n",
       "...              ...      ...       ...       ...      ...       ...   \n",
       "youthful   -0.535190  0.20648  0.465120 -0.115180 -0.12749 -0.375420   \n",
       "zeal       -0.890120  0.84982  0.060605 -0.132610 -0.28916  0.552470   \n",
       "zenith     -0.440520  0.50645 -1.026300 -0.219950  0.43890 -0.098213   \n",
       "zest       -0.815220 -0.47507 -0.279160  1.503500  0.10227  0.090781   \n",
       "zippy      -0.919060  0.12820 -0.329930 -0.176470  0.94167  0.356900   \n",
       "\n",
       "                  48        49  \n",
       "abound      0.223100 -0.926680  \n",
       "abounds     0.450880 -0.938780  \n",
       "abundance   0.094202 -0.321800  \n",
       "abundant   -0.080421 -0.523340  \n",
       "accessible  0.557920 -0.110500  \n",
       "...              ...       ...  \n",
       "youthful    0.066607 -0.142230  \n",
       "zeal        0.023004 -0.378880  \n",
       "zenith     -0.839860 -0.163000  \n",
       "zest       -0.087266 -0.076426  \n",
       "zippy       0.528820  0.706110  \n",
       "\n",
       "[1893 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vectors = embeddings.loc[embeddings.index.intersection(pos_words)]\n",
    "neg_vectors = embeddings.loc[embeddings.index.intersection(neg_words)]\n",
    "\n",
    "pos_vectors.sort_index(inplace = True)\n",
    "neg_vectors.sort_index(inplace = True)\n",
    "\n",
    "pos_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make arrays of the desired inputs and outputs. The inputs are the embeddings, and the outputs are 1 for positive words and -1 for negative words. We also make sure to keep track of the words they’re labeled with, so we can interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the input vectors, output values, and labels into training and test data, with 10% of the data used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our classifier, and train it by running the training vectors through it for 100 iterations. We use a logistic function as the loss as we're building a logistic regression model. The resulting classifier should output the probability that a word is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=100,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=0, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the classifier on the test vectors. It predicts the correct sentiment for sentiment words outside of its training data with around 88% accuracy. Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830128205128205"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define a function that we can use to see the sentiment that this classifier predicts for particular words, then use it to see some examples of its predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>2.953970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killed</th>\n",
       "      <td>-8.719976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peace</th>\n",
       "      <td>3.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attacks</th>\n",
       "      <td>-7.998912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>-0.366808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fall</th>\n",
       "      <td>-2.693952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popular</th>\n",
       "      <td>1.839639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>champion</th>\n",
       "      <td>3.299406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bomb</th>\n",
       "      <td>-6.058093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broke</th>\n",
       "      <td>-5.587792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protect</th>\n",
       "      <td>-1.050680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critics</th>\n",
       "      <td>-1.262191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lose</th>\n",
       "      <td>-0.347604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terror</th>\n",
       "      <td>-4.650521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>2.441816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offensive</th>\n",
       "      <td>-5.519903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot</th>\n",
       "      <td>-3.319520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stability</th>\n",
       "      <td>2.283495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgent</th>\n",
       "      <td>-1.894337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>-8.000703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentiment\n",
       "well        2.953970\n",
       "killed     -8.719976\n",
       "peace       3.983100\n",
       "attacks    -7.998912\n",
       "clear      -0.366808\n",
       "fall       -2.693952\n",
       "popular     1.839639\n",
       "champion    3.299406\n",
       "bomb       -6.058093\n",
       "broke      -5.587792\n",
       "protect    -1.050680\n",
       "critics    -1.262191\n",
       "lose       -0.347604\n",
       "terror     -4.650521\n",
       "approval    2.441816\n",
       "offensive  -5.519903\n",
       "plot       -3.319520\n",
       "stability   2.283495\n",
       "urgent     -1.894337\n",
       "abuse      -8.000703"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[embeddings.index.intersection(words)]\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than the accuracy number, this convinces us that the classifier is working. We can see that the classifier has learned to generalize sentiment to words outside of its training data. Note that the returned sentiment here is the result of `logprob(positive_class) - logprob(negative_class)`, and a positive value indicates positive sentiment and negative value indicates negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're reasonably happy with the classifier, we'll extend it to classify sentiment for a sentence. We can do so by simply computing the sentiment for each word in the sentence, and then taking the mean sentiment over all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test on some example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0584039704186323\n",
      "1.3265787691462831\n",
      "-0.3028293733994394\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sentiment(\"this example is pretty cool\"))\n",
    "print(text_to_sentiment(\"this example is okay\"))\n",
    "print(text_to_sentiment(\"meh, this example sucks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look pretty reasonable. Let's try more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.511555225196519\n",
      "0.45009422205976213\n",
      "-0.17230140715207987\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sentiment(\"Let's go get Italian food\"))\n",
    "print(text_to_sentiment(\"Let's go get Chinese food\"))\n",
    "print(text_to_sentiment(\"Let's go get Mexican food\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Mexican food seem to be associated with a negative sentiment. Let's try some names.\n",
    "\n",
    "Note: there will be randomness in terms of the output. Do not be alarmed if you see different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9727873384125275\n",
      "1.01529061284594\n",
      "1.0788585785392906\n",
      "-0.544038159419399\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sentiment(\"My name is Emily\"))\n",
    "print(text_to_sentiment(\"My name is Heather\"))\n",
    "print(text_to_sentiment(\"My name is Yvette\"))\n",
    "print(text_to_sentiment(\"My name is Yasin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the system has widely different sentiments with people's names. This is a little worrying. Did we just build a _racist_ sentiment classifier?\n",
    "\n",
    "Note: there will be randomness in terms of the output. Do not be alarmed if you see different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure this bias with a bit more rigour.\n",
    "\n",
    "Below we have four lists of names that tend to reflect different ethnic backgrounds. The first two are lists of predominantly “white” and “black” names adapted from [this paper](https://arxiv.org/pdf/1608.07187.pdf). We've also added typically Hispanic names, as well as Muslim names that come from Arabic or Urdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_BY_ETHNICITY = {\n",
    "    # The first two lists are from the Caliskan et al. appendix describing the\n",
    "    # Word Embedding Association Test.\n",
    "    'White': [\n",
    "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
    "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
    "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
    "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
    "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
    "        'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
    "        'Megan', 'Rachel', 'Wendy'\n",
    "    ],\n",
    "\n",
    "    'Black': [\n",
    "        'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
    "        'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
    "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
    "        'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
    "        'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
    "        'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
    "        'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
    "        'Tawanda', 'Yvette'\n",
    "    ],\n",
    "    \n",
    "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
    "    'Hispanic': [\n",
    "        'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
    "        'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
    "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
    "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
    "    ],\n",
    "    \n",
    "    # This list is compiled from baby-name sites for common Muslim names,\n",
    "    # as spelled in English.\n",
    "    # Note: the following list potentially conflates religion and ethnicity and so it isn't\n",
    "    # perfect.\n",
    "    'Arab/Muslim': [\n",
    "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
    "        'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
    "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
    "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
    "    ],\n",
    "    \n",
    "    # This list uses some of the most common Chinese given names\n",
    "    # (https://en.wikipedia.org/wiki/Chinese_given_name)\n",
    "    'Chinese': [\n",
    "        'Wei', 'Fang', 'Xiu Ying', 'Na', 'Min', 'Jing', 'Li', 'Qiang', 'Lei',\n",
    "        'Yang', 'Jie', 'Jun', 'Yong', 'Yan', \"Chao\", \"Tao\", \"Juan\", \"Han\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_sentiment_table():\n",
    "    frames = []\n",
    "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
    "        lower_names = [name.lower() for name in name_list]\n",
    "        sentiments = words_to_sentiment(lower_names)\n",
    "        sentiments['group'] = group\n",
    "        frames.append(sentiments)\n",
    "\n",
    "    # Put together the data we got from each ethnic group into one big table\n",
    "    return pd.concat(frames)\n",
    "\n",
    "name_sentiments = name_sentiment_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0.900462</td>\n",
       "      <td>Arab/Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ayaan</th>\n",
       "      <td>-3.833503</td>\n",
       "      <td>Arab/Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malika</th>\n",
       "      <td>-2.147892</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chao</th>\n",
       "      <td>-2.863590</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luisa</th>\n",
       "      <td>0.128827</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greg</th>\n",
       "      <td>0.413699</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amber</th>\n",
       "      <td>0.351407</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment        group\n",
       "ali      0.900462  Arab/Muslim\n",
       "ayaan   -3.833503  Arab/Muslim\n",
       "malika  -2.147892        Black\n",
       "chao    -2.863590      Chinese\n",
       "luisa    0.128827     Hispanic\n",
       "greg     0.413699        White\n",
       "amber    0.351407        White"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_sentiments.iloc[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO: code below could possibly be simplified here\n",
    "\n",
    "white_sentiments = name_sentiments[name_sentiments[\"group\"] == \"White\"][\"sentiment\"].tolist()\n",
    "black_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Black\"][\"sentiment\"].tolist()\n",
    "hispanic_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Hispanic\"][\"sentiment\"].tolist()\n",
    "arab_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Arab/Muslim\"][\"sentiment\"].tolist()\n",
    "chinese_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Chinese\"][\"sentiment\"].tolist()\n",
    "\n",
    "x = name_sentiments[\"sentiment\"].tolist()\n",
    "y= name_sentiments[\"group\"].tolist()\n",
    "\n",
    "plt.boxplot([white_sentiments, black_sentiments, hispanic_sentiments, arab_sentiments, chinese_sentiments],\n",
    "           labels = [\"White\", \"Black\", \"Hispanic\", \"Arab/Muslim\", \"Chinese\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the mean sentiment over these different ethnic groups, it's pretty clear that Black names are on average associated with negative sentiment, and so are Arab/Muslim and Chinese names (although not quite as negative).\n",
    "\n",
    "Why is this happening? Where is the source of bias? It's not from the sentiment lexicon, because it doesn't include any names. The source of bias comes from the pretrained GloVe word embeddings, which are trained using news and web data. As the data encodes biases and stereotypes that reflect our worldview, the sentiment classifier we built ultimately reflects that. It is perhaps impossible to have create perfectly neutral models or datasets, but the point here is **awareness**: that as engineers we should at least be wary of the biases of the data/models we develop, and documenting them in a way such that users/companies that use our systems know their limitations or weaknesses. Although we have yet to explore how to reduce bias, this exercise of building awareness constitutes the first step towards building ethical AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
